input {
  tcp {
    port => 50000
    codec => json
    type => "django"
  }
  file {
    path => "/var/log/postgres/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    # codec => line
    type => "postgres-log"
  }
  file {
    path => "/var/log/redis/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    # codec => line
    type => "redis-log"
  }
  file {
    path => "/var/log/vault/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    codec => json
    type => "vault-log"
  }
}

filter {
  if [type] == "django" {
    # Lowercase the level field and rename it to match the standard 'level' in Elasticsearch
    mutate {
      lowercase => ["levelname"]  # Convert 'levelname' to lowercase
      rename => { "levelname" => "level" }  # Rename for consistent index field name
    }

    # Parse the timestamp from Django to ensure it's used as @timestamp in Elasticsearch
    date {
      match => ["asctime", "YYYY-MM-dd HH:mm:ss"]
      target => "@timestamp"
      remove_field => ["asctime"]  # Clean up to avoid duplicate timestamp fields
    }
  }
  if [type] == "postgres-log" {
    # Parse the timestamp, PID, log level, and message using grok
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:timezone} \[%{NUMBER:pid}\] %{WORD:log_level}: %{GREEDYDATA:log_message}"
      }
    }
    # Optionally, parse the timestamp into a standard format for Elasticsearch
    date {
      match => ["timestamp", "YYYY-MM-dd HH:mm:ss.SSS"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
    # Optional: Add any other mutations or field renaming as necessary
    mutate {
      # Rename fields if necessary, e.g., rename 'log_level' to 'severity'
      rename => { "log_level" => "severity" }
      
      # Add any additional fields for indexing (like database or system info)
      add_field => { "log_source" => "postgres" }
    }
  }
  if [type] == "vault-log" {
    grok {
      match => {
        "message" => '^{"@level":"%{WORD:log_level}","@message":"(?<msg>[^"]*)","@module":"(?<module>[^"]*)","@timestamp":"(?<timestamp>[^"]*)".*$'
      }
    }

    # Convert timestamp to @timestamp field in Elasticsearch
    date {
      match => ["timestamp", "ISO8601"]
      remove_field => ["timestamp"]
    }

    # Rename fields for clarity
    mutate {
      rename => { "msg" => "message" }
    }
  }
  if [type] == "redis-log" {
    grok {
      match => { "message" => "%{NUMBER:pid}:%{WORD:role} %{DATE_US:date} %{TIME:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:message}" }
    }

    date {
      match => ["date", "dd MMM yyyy"]
      target => "@timestamp"
    }
  }
}

output {
  if [type] == "django" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "django-logs-%{+YYYY.MM.dd}"  # Adjusted to only include date in the index
      user => "elastic"  # Use environment variables for security
      password => "${ELASTIC_PASSWORD}"
      data_stream => "auto"
    }
  }
  if [type] == "postgres-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "postgres-logs-%{+YYYY.MM.dd}"  # New index for PostgreSQL logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      data_stream => "auto"
    }
  }
  if [type] == "vault-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "vault-logs-%{+YYYY.MM.dd}"  # New index for vault logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      data_stream => "auto"
    }
  }
  if [type] == "redis-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "redis-logs-%{+YYYY.MM.dd}"  # New index for redis logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      data_stream => "auto"
    }
  }
  # stdout {
  #   codec => rubydebug
  # }
}