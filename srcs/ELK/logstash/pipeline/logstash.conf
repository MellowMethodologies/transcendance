input {
  tcp {
    port => 50000
    codec => json
    type => "django"
  }
  file {
    path => "/var/log/postgres/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    codec => json
    type => "postgres-log"
  }
  file {
    path => "/var/log/vault/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    codec => json
    type => "vault-log"
  }
}

filter {
  if [type] == "django" {
    # Lowercase the level field and rename it to match the standard 'level' in Elasticsearch
    mutate {
      lowercase => ["levelname"]  # Convert 'levelname' to lowercase
      rename => { "levelname" => "level" }  # Rename for consistent index field name
    }

    # Parse the timestamp from Django to ensure it's used as @timestamp in Elasticsearch
    date {
      match => ["asctime", "YYYY-MM-dd HH:mm:ss"]
      target => "@timestamp"
      remove_field => ["asctime"]  # Clean up to avoid duplicate timestamp fields
    }
  }
  if [type] == "postgres-log" {
    # Parse the timestamp, PID, log level, and message using grok
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:timezone} \[%{NUMBER:pid}\] %{WORD:log_level}: %{GREEDYDATA:log_message}"
      }
    }
    # Optionally, parse the timestamp into a standard format for Elasticsearch
    date {
      match => ["timestamp", "YYYY-MM-dd HH:mm:ss.SSS"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
    # Optional: Add any other mutations or field renaming as necessary
    mutate {
      # Rename fields if necessary, e.g., rename 'log_level' to 'severity'
      rename => { "log_level" => "severity" }
      
      # Add any additional fields for indexing (like database or system info)
      add_field => { "log_source" => "postgres" }
    }
  }
  if [type] == "vault-log" {
    # Parse the timestamp, PID, log level, and message using grok
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:log_level}\]\s+(?<component>[A-Za-z0-9_-]+):\s+(?<message>.*)"
      }
    }

    # Optionally, you can parse the timestamp into @timestamp
    date {
      match => ["timestamp", "ISO8601"]
      remove_field => ["timestamp"]
    }
    # Optional: Add any other mutations or field renaming as necessary
    mutate {
      # Rename fields if necessary, e.g., rename 'log_level' to 'severity'
      rename => { "log_level" => "severity" }
      
      # Add any additional fields for indexing (like database or system info)
      add_field => { "log_source" => "vault" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "django-logs-%{+YYYY.MM.dd}"  # Adjusted to only include date in the index
    user => "elastic"  # Use environment variables for security
    password => "${ELASTIC_PASSWORD}"
  }
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "postgres-logs-%{+YYYY.MM.dd}"  # New index for PostgreSQL logs
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
  }
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "vault-logs-%{+YYYY.MM.dd}"  # New index for vault logs
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
  }
  # stdout {
  #   codec => rubydebug
  # }
}